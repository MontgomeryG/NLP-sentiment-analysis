{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta Model (huggingface)\n",
    "This model accounts for human language's dependance on the context between words.\n",
    "This is a \"transformer based deep learning model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "# Progress bar tracker\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# !pip install transformers\n",
    "# ! pip install --upgrade tensorflow\n",
    "# ! pip install --upgrade transformers\n",
    "# ! pip install --upgrade s3fs\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "# Softmax to apply to the output. We want to smooth the outputs between 0 and 1\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in model pretrained on data for sentiment\n",
    "# This model was trained on Twitter text.\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our positive and negative text.\n",
    "ex_good = \"This is so fun! Yay!\"\n",
    "ex_bad = \"I hate this so much.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roberta Model\n",
    "\n",
    "# encode text so model can understand\n",
    "# return_tensors set to 'pt' for 'pytorch'\n",
    "encoded_text = tokenizer(ex_bad, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2262, -0.8001, -2.4025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the TFAutoModelForSequenceClassification on our encoded_text\n",
    "output = model(**encoded_text)\n",
    "\n",
    "# Results are a tensor; a set of numbers representing points or vectors in 1 or multiple dimensions. \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.22616   , -0.80009806, -2.402533  ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the tensor output into numpy so we can store it\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97901547, 0.01746658, 0.00351787], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use softmax to scale the scores between 0 and 1\n",
    "scores = softmax(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.97901547, 'neu': 0.017466582, 'pos': 0.0035178654}\n"
     ]
    }
   ],
   "source": [
    "# The above output is given as [negative, neutral, positive]\n",
    "# let's create a dictionary that specifies the above for our scores\n",
    "scores_dict = {\n",
    "    'neg' : scores[0],\n",
    "    'neu' : scores[1],\n",
    "    'pos' : scores[2]\n",
    "}\n",
    "\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for creating polarity scores with Roberta\n",
    "\n",
    "def roberta_scorer(text):\n",
    "    encoded_text = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "    'neg' : scores[0],\n",
    "    'neu' : scores[1],\n",
    "    'pos' : scores[2]\n",
    "    }\n",
    "    return scores_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate this so much.---> Roberta Score: {'neg': 0.97901547, 'neu': 0.017466582, 'pos': 0.0035178654}\n",
      "This is so fun! Yay!---> Roberta Score: {'neg': 0.0017032464, 'neu': 0.005700617, 'pos': 0.9925962}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ex_bad}---> Roberta Score: {roberta_scorer(ex_bad)}\")\n",
    "print(f\"{ex_good}---> Roberta Score: {roberta_scorer(ex_good)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, when we loop the program, it will break if the output is too long. What we can do is put in an exception to skip the long ones, but....\n",
    "\n",
    "We want to do sentiment analysis on rather long texts, such as our press releases.\n",
    "\n",
    "Let's bring in our press release information and the roberta_scorer on a few examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>description</th>\n",
       "      <th>sector</th>\n",
       "      <th>headline</th>\n",
       "      <th>full text</th>\n",
       "      <th>location released</th>\n",
       "      <th>market reaction(1 buy, 0 forego)</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 29 2024</td>\n",
       "      <td>PALI</td>\n",
       "      <td>Palisade Bio</td>\n",
       "      <td>biopharmaceuticals</td>\n",
       "      <td>Palisade Bio Announces Positive Preclinical Da...</td>\n",
       "      <td>Preclinical data demonstrated PALI-2108 to be ...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 30, 2024</td>\n",
       "      <td>PXLW</td>\n",
       "      <td>Pixelworks</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Walt Disney Studios and Pixelworks Enter into ...</td>\n",
       "      <td>Disney will pioneer motion grading on select t...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 30, 2024</td>\n",
       "      <td>ITRM</td>\n",
       "      <td>Iterum Therapeutics</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Iterum Therapeutics Announces Positive Topline...</td>\n",
       "      <td>Phase 3 REASSURE Trial Met Primary Endpoint of...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 30, 2024</td>\n",
       "      <td>MARK</td>\n",
       "      <td>Remark Holdings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remark Holdings On X Posts \"Looking forward to...</td>\n",
       "      <td>This is fake text</td>\n",
       "      <td>Xtwitter</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 31, 2024</td>\n",
       "      <td>ENSC</td>\n",
       "      <td>Ensysce Biosciences</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Ensysce Biosciences Announces Positive End of ...</td>\n",
       "      <td>PF614 Phase 3 Program Expected to Begin Enroll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jan 31, 2024</td>\n",
       "      <td>DCGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>DocGo Announces Share Buyback Program</td>\n",
       "      <td>DocGo Announces Share Buyback Program\\nJanuary...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Feb 1, 2024</td>\n",
       "      <td>NRBO</td>\n",
       "      <td>Neurobo Pharmaceuticals</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>NEUROBO PHARMACEUTICALS ANNOUNCES FDA CLEARANC...</td>\n",
       "      <td>FEB 1, 2024 AT 8:01 AM EST\\nDownload PDF\\nPrec...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feb 1, 2024</td>\n",
       "      <td>CMND</td>\n",
       "      <td>Clearmind Medicine</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Clearmind Medicine Announces Exclusive, Long-T...</td>\n",
       "      <td>The licensing agreement refers to the company'...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Feb 2, 2024</td>\n",
       "      <td>INBS</td>\n",
       "      <td>Intellegent Bio Solutions</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>INTELLIGENT BIO SOLUTIONS INC. ANNOUNCES PRELI...</td>\n",
       "      <td>YEAR-OVER-YEAR UNAUDITED REVENUE INCREASED 114...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>730000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Feb 2, 2024</td>\n",
       "      <td>KOPN</td>\n",
       "      <td>Kopin Corporation</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>KOPIN RECEIVES $3 MILLION IN NEW ORDERS FOR SE...</td>\n",
       "      <td>WESTBOROUGH, Mass. --(BUSINESS WIRE)--  Kopin ...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Feb 2, 2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Revance Receives Permanent J-Code for DAXXIFY®...</td>\n",
       "      <td>NASHVILLE, Tenn.--(BUSINESS WIRE)-- Revance Th...</td>\n",
       "      <td>IR website</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date stock                description                  sector  \\\n",
       "0    Jan 29 2024  PALI               Palisade Bio      biopharmaceuticals   \n",
       "1   Jan 30, 2024  PXLW                 Pixelworks  Information Technology   \n",
       "2   Jan 30, 2024  ITRM        Iterum Therapeutics              Healthcare   \n",
       "3   Jan 30, 2024  MARK            Remark Holdings                     NaN   \n",
       "4   Jan 31, 2024  ENSC        Ensysce Biosciences              Healthcare   \n",
       "5   Jan 31, 2024  DCGO                        NaN              Healthcare   \n",
       "6    Feb 1, 2024  NRBO    Neurobo Pharmaceuticals              Healthcare   \n",
       "7    Feb 1, 2024  CMND         Clearmind Medicine              Healthcare   \n",
       "8    Feb 2, 2024  INBS  Intellegent Bio Solutions              Healthcare   \n",
       "9    Feb 2, 2024  KOPN          Kopin Corporation  Information Technology   \n",
       "10   Feb 2, 2024   NaN                        NaN                     NaN   \n",
       "\n",
       "                                             headline  \\\n",
       "0   Palisade Bio Announces Positive Preclinical Da...   \n",
       "1   Walt Disney Studios and Pixelworks Enter into ...   \n",
       "2   Iterum Therapeutics Announces Positive Topline...   \n",
       "3   Remark Holdings On X Posts \"Looking forward to...   \n",
       "4   Ensysce Biosciences Announces Positive End of ...   \n",
       "5               DocGo Announces Share Buyback Program   \n",
       "6   NEUROBO PHARMACEUTICALS ANNOUNCES FDA CLEARANC...   \n",
       "7   Clearmind Medicine Announces Exclusive, Long-T...   \n",
       "8   INTELLIGENT BIO SOLUTIONS INC. ANNOUNCES PRELI...   \n",
       "9   KOPIN RECEIVES $3 MILLION IN NEW ORDERS FOR SE...   \n",
       "10  Revance Receives Permanent J-Code for DAXXIFY®...   \n",
       "\n",
       "                                            full text location released  \\\n",
       "0   Preclinical data demonstrated PALI-2108 to be ...        IR website   \n",
       "1   Disney will pioneer motion grading on select t...        IR website   \n",
       "2   Phase 3 REASSURE Trial Met Primary Endpoint of...        IR website   \n",
       "3                                   This is fake text          Xtwitter   \n",
       "4   PF614 Phase 3 Program Expected to Begin Enroll...               NaN   \n",
       "5   DocGo Announces Share Buyback Program\\nJanuary...        IR website   \n",
       "6   FEB 1, 2024 AT 8:01 AM EST\\nDownload PDF\\nPrec...        IR website   \n",
       "7   The licensing agreement refers to the company'...        IR website   \n",
       "8   YEAR-OVER-YEAR UNAUDITED REVENUE INCREASED 114...        IR website   \n",
       "9   WESTBOROUGH, Mass. --(BUSINESS WIRE)--  Kopin ...        IR website   \n",
       "10  NASHVILLE, Tenn.--(BUSINESS WIRE)-- Revance Th...        IR website   \n",
       "\n",
       "    market reaction(1 buy, 0 forego)    shares  \n",
       "0                                  1       NaN  \n",
       "1                                  1       NaN  \n",
       "2                                  1       NaN  \n",
       "3                                  1       NaN  \n",
       "4                                  1       NaN  \n",
       "5                                  1       NaN  \n",
       "6                                  1       NaN  \n",
       "7                                  1       NaN  \n",
       "8                                  1  730000.0  \n",
       "9                                  1       NaN  \n",
       "10                                 0       NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "releases_df = pd.read_excel(\"../sentiment-analysis/resources/realStockNewsReleases.xlsx\")\n",
    "\n",
    "releases_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model we are using has not been trained on news release information, it will be interesting to see how it scores these press releases.\n",
    "I added my own scores of 1 or 0 to indicate whether or not we should risk implementing our strategy on similar news catalysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Palisade Bio Announces Positive Preclinical Data of Lead Program PALI-2108 at the 2024 Crohn’s & Colitis Congress'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try a headline first.\n",
    "\n",
    "headline = releases_df[\"headline\"][0]\n",
    "headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0025271582, 'neu': 0.34376016, 'pos': 0.6537127}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# According to our personal analysis, the score should be relatively positive.\n",
    "roberta_scorer(headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Let's run it on all headlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_score = releases_df[\"market reaction(1 buy, 0 forego)\"][0]\n",
    "human_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Palisade Bio Announces Positive Preclinical Data of Lead Program PALI-2108 at the 2024 Crohn’s & Colitis Congress'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = releases_df[\"headline\"][0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palisade Bio Announces Positive Preclinical Data of Lead Program PALI-2108 at the 2024 Crohn’s & Colitis Congress________{'neg': 0.0025271582, 'neu': 0.34376016, 'pos': 0.6537127}________Human Score: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walt Disney Studios and Pixelworks Enter into a First of its Kind Multi-Year Agreement to Expand Reach of TrueCut Motion Technology________{'neg': 0.0023783133, 'neu': 0.61753714, 'pos': 0.38008454}________Human Score: 1\n",
      "Iterum Therapeutics Announces Positive Topline Results from its Phase 3 REASSURE Clinical Trial of Oral Sulopenem in Uncomplicated Urinary Tract Infections________{'neg': 0.0028943564, 'neu': 0.25223637, 'pos': 0.7448693}________Human Score: 1\n",
      "Remark Holdings On X Posts \"Looking forward to partnering with Microsoft Azure On RemarkAI computervision largevisionmodels globally.\" Remark Holdings Earlier Posted On X \"It's official!!! Microsoft making RemarkAI global together - $80 million dollar initial partnership\"________{'neg': 0.00092027907, 'neu': 0.06833758, 'pos': 0.9307422}________Human Score: 1\n",
      "Ensysce Biosciences Announces Positive End of Phase 2 Meeting with FDA for PF614 to Treat Severe Pain________{'neg': 0.0037650187, 'neu': 0.46445823, 'pos': 0.5317767}________Human Score: 1\n",
      "DocGo Announces Share Buyback Program________{'neg': 0.023693265, 'neu': 0.9130483, 'pos': 0.063258454}________Human Score: 1\n",
      "NEUROBO PHARMACEUTICALS ANNOUNCES FDA CLEARANCE OF IND FOR A PHASE 1 CLINICAL TRIAL OF DA-1726 FOR THE TREATMENT OF OBESITY________{'neg': 0.030394834, 'neu': 0.9138663, 'pos': 0.055738907}________Human Score: 1\n",
      "Clearmind Medicine Announces Exclusive, Long-Term Licensing Agreement with Leading Israeli Research Center________{'neg': 0.0072500384, 'neu': 0.86446005, 'pos': 0.12828994}________Human Score: 1\n",
      "INTELLIGENT BIO SOLUTIONS INC. ANNOUNCES PRELIMINARY UNAUDITED FISCAL SECOND QUARTER AND SIX-MONTH REVENUE RESULTS________{'neg': 0.04189578, 'neu': 0.8802485, 'pos': 0.077855684}________Human Score: 1\n",
      "KOPIN RECEIVES $3 MILLION IN NEW ORDERS FOR SEVERAL SIMULATED THERMAL SIGHTS________{'neg': 0.017655602, 'neu': 0.7493605, 'pos': 0.23298389}________Human Score: 1\n",
      "Revance Receives Permanent J-Code for DAXXIFY® and Announces Publication of DAXXIFY Pivotal Study (ASPEN-1) Results in Neurology®________{'neg': 0.009080967, 'neu': 0.8820797, 'pos': 0.10883931}________Human Score: 0\n"
     ]
    }
   ],
   "source": [
    "y = 0\n",
    "for x in releases_df[\"headline\"]:\n",
    "    score = roberta_scorer(x)\n",
    "    human_score = releases_df[\"market reaction(1 buy, 0 forego)\"][y]\n",
    "    print(f\"{x}________{score}________Human Score: {human_score}\")\n",
    "    y+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the program doesn't seem understand how some of the technical financial terms would relate to whether or not we should long the stock after the particular headline comes out.\n",
    "\n",
    "For example, the Roberta model gave us a 0.9130483 neutral score for a headline indicating a share buyback program. Based on my personal knowledge of the stock market, a share buy back program generally leads to a increase in stock price, since the demand is increasing. Therefore, I hypothesize that if our model labeled the buyback program as more positive than neutral, it would be a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1060) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1060].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m releases_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull text\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     score \u001b[38;5;241m=\u001b[39m roberta_scorer(x)\n\u001b[1;32m      5\u001b[0m     human_score \u001b[38;5;241m=\u001b[39m releases_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarket reaction(1 buy, 0 forego)\u001b[39m\u001b[38;5;124m\"\u001b[39m][y]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m________\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m________Human Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhuman_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m, in \u001b[0;36mroberta_scorer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroberta_scorer\u001b[39m(text):\n\u001b[1;32m      4\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_text)\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m     scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:801\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    800\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 801\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m buffered_token_type_ids\u001b[38;5;241m.\u001b[39mexpand(batch_size, seq_length)\n\u001b[1;32m    802\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1060) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1060].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "# Let's try to run it on the full text.\n",
    "y = 0\n",
    "for x in releases_df[\"full text\"]:\n",
    "    score = roberta_scorer(x)\n",
    "    human_score = releases_df[\"market reaction(1 buy, 0 forego)\"][y]\n",
    "    print(f\"{x}________{score}________Human Score: {human_score}\")\n",
    "    y+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are getting the error that the size of the text is too long. \n",
    "\n",
    "Let's throw in an exception that skips the \"Full text\"s that are too long. We will likely skip them all, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Broke for this one\n",
      " Broke for this one\n",
      " Broke for this one\n",
      "This is fake text________{'neg': 0.80862635, 'neu': 0.1813547, 'pos': 0.010019027}________Human Score: 1\n",
      " Broke for this one\n",
      " Broke for this one\n",
      " Broke for this one\n",
      " Broke for this one\n",
      " Broke for this one\n",
      "WESTBOROUGH, Mass. --(BUSINESS WIRE)--  Kopin Corporation  (NASDAQ: KOPN), a leading provider of application-specific optical solutions and high performance micro-displays for defense, enterprise, consumer and medical products, today announced it has received several new orders from multiple customers for its simulated thermal sights used in armored vehicle training systems, totaling in excess of  $3 million . These simulated thermal sights support the training of armored vehicle crews in integrated multi-platform mission trainers. We believe recent deployment of armored vehicles in multiple theaters is driving the need for increased training capabilities and leading to the procurement of significant quantities of additional trainers.\n",
      "“Kopin has a long history of supplying advanced training systems that create high fidelity simulated situations that the crews experience in fielded armored vehicles,” stated  Bill Maffucci , the Company’s Senior Vice President of Business Development and Strategy. “Our supply of both the tactical and training versions of these display systems allows Kopin to accurately reproduce the look and feel for training systems, resulting in maximum effectiveness when training our warfighters. We are very pleased to have received these additional orders and recognize the trust placed in Kopin in the very important task of preparing or men and women to defend our freedom.”\n",
      "The Kopin products supplied under these orders are fully integrated display systems consisting of microdisplays, optics, electronics and environmental enclosures which simulate the display Line Replaceable Units (LRUs) installed in several  U.S.  manufactured armored vehicles. Deliveries of these systems are scheduled to take place through mid-2024.________{'neg': 0.003914964, 'neu': 0.18337178, 'pos': 0.8127132}________Human Score: 1\n",
      " Broke for this one\n"
     ]
    }
   ],
   "source": [
    "# Let's try to run it on the full text.\n",
    "y = 0\n",
    "for x in releases_df[\"full text\"]:\n",
    "    try:\n",
    "        score = roberta_scorer(x)\n",
    "        human_score = releases_df[\"market reaction(1 buy, 0 forego)\"][y]\n",
    "        print(f\"{x}________{score}________Human Score: {human_score}\")\n",
    "        y+=1\n",
    "    except RuntimeError:\n",
    "        print(f' Broke for this one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the Roberta model from Hugging face will not be able to adequately label the sentiment of these news releases. \n",
    "\n",
    "Let's search the web for models that were trained on stocks, not people's public thoughs gather from Twitter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
